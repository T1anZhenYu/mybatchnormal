{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean size: torch.Size([3])\n",
      "variance size: torch.Size([3])\n",
      "running_mean size: torch.Size([3])\n",
      "running_var size: torch.Size([3])\n",
      "train: True\n",
      "mean size: torch.Size([3])\n",
      "variance size: torch.Size([3])\n",
      "running_mean size: torch.Size([3])\n",
      "running_var size: torch.Size([3])\n",
      "train: True\n",
      "mean size: torch.Size([3])\n",
      "variance size: torch.Size([3])\n",
      "running_mean size: torch.Size([3])\n",
      "running_var size: torch.Size([3])\n",
      "train: True\n",
      "mean size: torch.Size([3])\n",
      "variance size: torch.Size([3])\n",
      "running_mean size: torch.Size([3])\n",
      "running_var size: torch.Size([3])\n",
      "train: True\n",
      "mean size: torch.Size([3])\n",
      "variance size: torch.Size([3])\n",
      "running_mean size: torch.Size([3])\n",
      "running_var size: torch.Size([3])\n",
      "train: True\n",
      "mean size: torch.Size([3])\n",
      "variance size: torch.Size([3])\n",
      "running_mean size: torch.Size([3])\n",
      "running_var size: torch.Size([3])\n",
      "train: True\n",
      "mean size: torch.Size([3])\n",
      "variance size: torch.Size([3])\n",
      "running_mean size: torch.Size([3])\n",
      "running_var size: torch.Size([3])\n",
      "train: True\n",
      "mean size: torch.Size([3])\n",
      "variance size: torch.Size([3])\n",
      "running_mean size: torch.Size([3])\n",
      "running_var size: torch.Size([3])\n",
      "train: True\n",
      "mean size: torch.Size([3])\n",
      "variance size: torch.Size([3])\n",
      "running_mean size: torch.Size([3])\n",
      "running_var size: torch.Size([3])\n",
      "train: True\n",
      "mean size: torch.Size([3])\n",
      "variance size: torch.Size([3])\n",
      "running_mean size: torch.Size([3])\n",
      "running_var size: torch.Size([3])\n",
      "train: True\n",
      "All parameters are equal!\n",
      "True\n",
      "Max diff:  tensor(1.4305e-06, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Modified from https://github.com/ptrblck/pytorch_misc/blob/master/batch_norm_manual.py\n",
    "\n",
    "@author: Xin Dong\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyBatchNorm2d(nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1,\n",
    "                 affine=True, track_running_stats=True):\n",
    "        super(MyBatchNorm2d, self).__init__(\n",
    "            num_features, eps, momentum, affine, track_running_stats)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self._check_input_dim(input)\n",
    "\n",
    "        exponential_average_factor = 0.0\n",
    "\n",
    "        if self.training and self.track_running_stats:\n",
    "            if self.num_batches_tracked is not None:\n",
    "                self.num_batches_tracked += 1\n",
    "                if self.momentum is None:  # use cumulative moving average\n",
    "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
    "                else:  # use exponential moving average\n",
    "                    exponential_average_factor = self.momentum\n",
    "\n",
    "        # calculate running estimates\n",
    "        if self.training:\n",
    "            mean = input.mean([0, 2, 3])\n",
    "            print('mean size:', mean.size())\n",
    "            # use biased var in train\n",
    "            var = input.var([0, 2, 3], unbiased=False)\n",
    "            print('variance size:', var.size())\n",
    "            n = input.numel() / input.size(1)\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = exponential_average_factor * mean\\\n",
    "                    + (1 - exponential_average_factor) * self.running_mean\n",
    "                print('running_mean size:', self.running_mean.size())\n",
    "                # update running_var with unbiased var\n",
    "                self.running_var = exponential_average_factor * var * n / (n - 1)\\\n",
    "                    + (1 - exponential_average_factor) * self.running_var\n",
    "                print('running_var size:', self.running_var.size())\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        input = (input - mean[None, :, None, None]) / (torch.sqrt(var[None, :, None, None] + self.eps))\n",
    "        if self.affine:\n",
    "            input = input * self.weight[None, :, None, None] + self.bias[None, :, None, None]\n",
    "\n",
    "        return input\n",
    "    \n",
    "    \n",
    "    \n",
    "def compare_bn(bn1, bn2):\n",
    "    err = False\n",
    "    if not torch.allclose(bn1.running_mean, bn2.running_mean):\n",
    "        print('Diff in running_mean: {} vs {}'.format(\n",
    "            bn1.running_mean, bn2.running_mean))\n",
    "        err = True\n",
    "\n",
    "    if not torch.allclose(bn1.running_var, bn2.running_var):\n",
    "        print('Diff in running_var: {} vs {}'.format(\n",
    "            bn1.running_var, bn2.running_var))\n",
    "        err = True\n",
    "\n",
    "    if bn1.affine and bn2.affine:\n",
    "        if not torch.allclose(bn1.weight, bn2.weight):\n",
    "            print('Diff in weight: {} vs {}'.format(\n",
    "                bn1.weight, bn2.weight))\n",
    "            err = True\n",
    "\n",
    "        if not torch.allclose(bn1.bias, bn2.bias):\n",
    "            print('Diff in bias: {} vs {}'.format(\n",
    "                bn1.bias, bn2.bias))\n",
    "            err = True\n",
    "\n",
    "    if not err:\n",
    "        print('All parameters are equal!')\n",
    "        \n",
    "        \n",
    "my_bn = MyBatchNorm2d(3, affine=True)\n",
    "bn = nn.BatchNorm2d(3, affine=True)\n",
    "\n",
    "my_bn.train()\n",
    "bn.train()\n",
    "for i in range(10):\n",
    "    x = torch.randn(10, 3, 100, 100)\n",
    "    out1 = my_bn(x)\n",
    "    out2 = bn(x)\n",
    "    print('train:', torch.allclose(out1, out2))\n",
    "    \n",
    "compare_bn(my_bn, bn)\n",
    "\n",
    "my_bn.eval()\n",
    "bn.eval()\n",
    "x = torch.randn(10, 3, 100, 100)\n",
    "out1 = my_bn(x)\n",
    "out2 = bn(x)\n",
    "print(torch.allclose(out1, out2))\n",
    "print('Max diff: ', (out1 - out2).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parameters are equal!\n",
      "All parameters are equal!\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(7.1526e-07, grad_fn=<MaxBackward1>)\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(7.1526e-07, grad_fn=<MaxBackward1>)\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(1.4305e-06, grad_fn=<MaxBackward1>)\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(9.5367e-07, grad_fn=<MaxBackward1>)\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(7.1526e-07, grad_fn=<MaxBackward1>)\n",
      "**: tensor(-27.1325) tensor(8.6804)\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
      "**: tensor(-43.7317) tensor(39.4106)\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(9.5367e-07, grad_fn=<MaxBackward1>)\n",
      "**: tensor(3.5037) tensor(12.2386)\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(2.3842e-07, grad_fn=<MaxBackward1>)\n",
      "**: tensor(-42.4175) tensor(42.2621)\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(9.5367e-07, grad_fn=<MaxBackward1>)\n",
      "**: tensor(-15.1669) tensor(22.7972)\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
      "**: tensor(-38.4294) tensor(32.2655)\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(9.5367e-07, grad_fn=<MaxBackward1>)\n",
      "**: tensor(-34.8359) tensor(47.2662)\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(9.5367e-07, grad_fn=<MaxBackward1>)\n",
      "**: tensor(-8.3711) tensor(10.4201)\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(2.3842e-07, grad_fn=<MaxBackward1>)\n",
      "**: tensor(-30.9317) tensor(25.5652)\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
      "**: tensor(-3.7959) tensor(14.6309)\n",
      "All parameters are equal!\n",
      "Max diff:  tensor(2.3842e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Init BatchNorm layers\n",
    "my_bn = MyBatchNorm2d(3, affine=True)\n",
    "bn = nn.BatchNorm2d(3, affine=True)\n",
    "\n",
    "compare_bn(my_bn, bn)  # weight and bias should be different\n",
    "# Load weight and bias\n",
    "my_bn.load_state_dict(bn.state_dict())\n",
    "compare_bn(my_bn, bn)\n",
    "\n",
    "# Run train\n",
    "for _ in range(10):\n",
    "    scale = torch.randint(1, 10, (1,)).float()\n",
    "    bias = torch.randint(-10, 10, (1,)).float()\n",
    "    x = torch.randn(10, 3, 100, 100) * scale + bias\n",
    "    out1 = my_bn(x)\n",
    "    out2 = bn(x)\n",
    "    compare_bn(my_bn, bn)\n",
    "\n",
    "    torch.allclose(out1, out2)\n",
    "    print('Max diff: ', (out1 - out2).abs().max())\n",
    "\n",
    "# Run eval\n",
    "my_bn.eval()\n",
    "bn.eval()\n",
    "for _ in range(10):\n",
    "    scale = torch.randint(1, 10, (1,)).float()\n",
    "    bias = torch.randint(-10, 10, (1,)).float()\n",
    "    x = torch.randn(10, 3, 100, 100) * scale + bias\n",
    "    print('**:', x.min(), x.max())\n",
    "    out1 = my_bn(x)\n",
    "    out2 = bn(x)\n",
    "    compare_bn(my_bn, bn)\n",
    "\n",
    "    torch.allclose(out1, out2)\n",
    "    print('Max diff: ', (out1 - out2).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean size: torch.Size([10, 3])\n",
      "variance size: torch.Size([3])\n",
      "running_mean size: torch.Size([3])\n",
      "running_var size: torch.Size([3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (100) must match the size of tensor b (3) at non-singleton dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d91e3d3bc9db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0msm_bn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSM_BatchNorm2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maffine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mout_sm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-d91e3d3bc9db>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#         input = (input - mean[None, :, None, None]) / (torch.sqrt(var[None, :, None, None] + self.eps))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (100) must match the size of tensor b (3) at non-singleton dimension 4"
     ]
    }
   ],
   "source": [
    "class SM_BatchNorm2d(nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1,\n",
    "                 affine=True, track_running_stats=True):\n",
    "        super(SM_BatchNorm2d, self).__init__(\n",
    "            num_features, eps, momentum, affine, track_running_stats)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self._check_input_dim(input)\n",
    "\n",
    "        exponential_average_factor = 0.0\n",
    "\n",
    "        if self.training and self.track_running_stats:\n",
    "            if self.num_batches_tracked is not None:\n",
    "                self.num_batches_tracked += 1\n",
    "                if self.momentum is None:  # use cumulative moving average\n",
    "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
    "                else:  # use exponential moving average\n",
    "                    exponential_average_factor = self.momentum\n",
    "\n",
    "        # calculate running estimates\n",
    "        if self.training:\n",
    "            mean = input.mean([2, 3])\n",
    "            print('mean size:', mean.size())\n",
    "            # use biased var in train\n",
    "            var = input.var([0, 2, 3], unbiased=False)\n",
    "            print('variance size:', var.size())\n",
    "            n = input.numel() / input.size(1)\n",
    "            with torch.no_grad():\n",
    "                for i in range(mean.size(0)):\n",
    "                    self.running_mean = exponential_average_factor * mean[i]\\\n",
    "                        + (1-exponential_average_factor) * self.running_mean\n",
    "                print('running_mean size:', self.running_mean.size())\n",
    "#                 self.running_mean = exponential_average_factor * mean\\\n",
    "#                     + (1 - exponential_average_factor) * self.running_mean\n",
    "                # update running_var with unbiased var\n",
    "                self.running_var = exponential_average_factor * var * n / (n - 1)\\\n",
    "                    + (1 - exponential_average_factor) * self.running_var\n",
    "                print('running_var size:', self.running_var.size())\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "#         input = (input - mean[None, :, None, None]) / (torch.sqrt(var[None, :, None, None] + self.eps))\n",
    "        input = (input - mean[None, :, None, None]) / (torch.sqrt(var[None, :, None, None] + self.eps))\n",
    "        if self.affine:\n",
    "            input = input * self.weight[None, :, None, None] + self.bias[None, :, None, None]\n",
    "\n",
    "        return input\n",
    "    \n",
    "    \n",
    "sm_bn = SM_BatchNorm2d(3, affine=True)\n",
    "x = torch.randn(10, 3, 100, 100)\n",
    "out_sm = sm_bn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean size: torch.Size([3])\n",
      "variance size: torch.Size([10, 3])\n"
     ]
    }
   ],
   "source": [
    "class SV_BatchNorm2d(nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1,\n",
    "                 affine=True, track_running_stats=True):\n",
    "        super(SV_BatchNorm2d, self).__init__(\n",
    "            num_features, eps, momentum, affine, track_running_stats)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self._check_input_dim(input)\n",
    "\n",
    "        exponential_average_factor = 0.0\n",
    "\n",
    "        if self.training and self.track_running_stats:\n",
    "            if self.num_batches_tracked is not None:\n",
    "                self.num_batches_tracked += 1\n",
    "                if self.momentum is None:  # use cumulative moving average\n",
    "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
    "                else:  # use exponential moving average\n",
    "                    exponential_average_factor = self.momentum\n",
    "\n",
    "        # calculate running estimates\n",
    "        if self.training:\n",
    "            mean = input.mean([0, 2, 3])\n",
    "            print('mean size:', mean.size())\n",
    "            # use biased var in train\n",
    "            var = input.var([2, 3], unbiased=False)\n",
    "            print('variance size:', var.size())\n",
    "            n = input.numel() / (input.size(1) * input.size(0))\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = exponential_average_factor * mean\\\n",
    "                    + (1 - exponential_average_factor) * self.running_mean\n",
    "                # update running_var with unbiased var\n",
    "                for i in range(var.size(0)):\n",
    "                    self.running_var = exponential_average_factor * var[i] * n / (n - 1)\\\n",
    "                        + (1 - exponential_average_factor) * self.running_var\n",
    "#                 self.running_var = exponential_average_factor * var * n / (n - 1)\\\n",
    "#                     + (1 - exponential_average_factor) * self.running_var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "#         input = (input - mean[None, :, None, None]) / (torch.sqrt(var[None, :, None, None] + self.eps))\n",
    "        input = (input - mean[None, :, None, None]) / (torch.sqrt(var[:, :, None, None] + self.eps))\n",
    "        if self.affine:\n",
    "            input = input * self.weight[None, :, None, None] + self.bias[None, :, None, None]\n",
    "\n",
    "        return input\n",
    "    \n",
    "    \n",
    "sv_bn = SV_BatchNorm2d(3, affine=True)\n",
    "x = torch.randn(10, 3, 100, 100)\n",
    "out_sv = sv_bn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm2d(nn.BatchNorm2d):\n",
    "    def forward(self, x):\n",
    "        self._check_input_dim(x)\n",
    "        y = x.transpose(0,1)\n",
    "        return_shape = y.shape\n",
    "        y = y.contiguous().view(x.size(1), -1)\n",
    "        mu = y.mean(dim=1)\n",
    "        sigma2 = y.var(dim=1)\n",
    "        if self.training is not True:\n",
    "            y = y - self.running_mean.view(-1, 1)\n",
    "            y = y / (self.running_var.view(-1, 1)**.5 + self.eps)\n",
    "        else:\n",
    "            if self.track_running_stats is True:\n",
    "                with torch.no_grad():\n",
    "                    self.running_mean = (1-self.momentum)*self.running_mean + self.momentum*mu\n",
    "                    self.running_var = (1-self.momentum)*self.running_var + self.momentum*sigma2\n",
    "            y = y - mu.view(-1,1)\n",
    "            y = y / (sigma2.view(-1,1)**.5 + self.eps)\n",
    "\n",
    "        y = self.weight.view(-1, 1) * y + self.bias.view(-1, 1)\n",
    "        return y.view(return_shape).transpose(0,1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
